{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import re, array, csv, datetime, glob, json, math, random, stat\n",
    "import pytz, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "# This is a utility function for running other ipython notebooks\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "# Load utility functions from another ipython notebook\n",
    "root_dir = \"/projects/9ab71616-fcde-4524-bf8f-7953c669ebbb/air-src/\"\n",
    "os.chdir(root_dir + \"linRegModel/pardump_example/\")\n",
    "exec_ipynb(\"pardumpdump-randy-amy-util.ipynb\")\n",
    "os.chdir(root_dir + \"linRegModel\")\n",
    "exec_ipynb(\"./cachedHysplitRunLib.ipynb\")\n",
    "exec_ipynb(\"../../src/python-utils/utils.ipynb\")\n",
    "os.chdir(root_dir + \"automate-plume-viz/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Generate the EarthTime layers and the thumbnail server urls\n",
    "# These urls can be called later to obtain video frames\n",
    "# Input:\n",
    "#   start_date_eastern: the date to start in EST time, e.g., \"2019-01-01\"\n",
    "#   end_date_eastern: the date to start in EST time, e.g., \"2020-01-01\"\n",
    "#   offset_hour: time offset in hours, for example, if this is 3, then it starts from 12-3=9 p.m. instead of 12 a.m.\n",
    "#   url_partition: the number of partitions for the thumbnail server request for getting images of video frames\n",
    "# Output:\n",
    "#   df_layer: the pandas dataframe for the EarthTime layer document\n",
    "#   df_share_url: the pandas dataframe for the share urls\n",
    "#   df_img_url: the pandas dataframe for the thumbnail server urls to get images of video frames\n",
    "#   start_d: a pandas series of the starting datetime object in EST time\n",
    "#   file_name: a list of file names\n",
    "def generate_metadata(start_date_eastern, end_date_eastern, offset_hours=3, url_partition=6):\n",
    "    # Create rows in the EarthTime layer document\n",
    "    offset_d = pd.Timedelta(offset_hours, unit=\"h\")\n",
    "    start_d = pd.date_range(start=start_date_eastern, end=end_date_eastern, closed=\"left\", tz=\"US/Eastern\") - offset_d\n",
    "    end_d = pd.date_range(start=start_date_eastern, end=end_date_eastern, closed=\"right\", tz=\"US/Eastern\") - offset_d\n",
    "    df_template = pd.read_csv(\"data/earth_time_template.csv\")\n",
    "    df_layer = pd.concat([df_template]*len(start_d), ignore_index=True)\n",
    "    file_name = \"plume_\" + end_d.strftime(\"%Y%m%d\")\n",
    "    start_d_utc = start_d.tz_convert(\"UTC\")\n",
    "    end_d_utc = end_d.tz_convert(\"UTC\")\n",
    "    df_layer[\"Start date\"] = start_d_utc.strftime(\"%Y%m%d%H%M%S\")\n",
    "    df_layer[\"End date\"] = end_d_utc.strftime(\"%Y%m%d%H%M%S\")\n",
    "    df_layer[\"Share link identifier\"] = file_name\n",
    "    df_layer[\"Name\"] = \"PARDUMP \" + end_d.strftime(\"%Y-%m-%d\")\n",
    "    df_layer[\"URL\"] = \"https://cocalc-www.createlab.org/pardumps/\" + file_name + \".bin\"\n",
    "\n",
    "    # Create rows of share URLs\n",
    "    et_root_url = \"https://davos2019.earthtime.org/explore#\"\n",
    "    et_part = \"v=581806,708156,584252,710601,pts&ps=2400&startDwell=0&endDwell=0\"\n",
    "    ts_root_url = \"https://thumbnails-earthtime.cmucreatelab.org/thumbnail?\"\n",
    "    ts_part = \"&width=480&height=480&format=zip&fps=30&tileFormat=mp4&startDwell=0&endDwell=0&fromScreenshot&disableUI\"\n",
    "    share_url_ls = [] # EarthTime share urls\n",
    "    dt_share_url_ls = [] # the date of the share urls\n",
    "    img_url_ls = [] # thumbnail server urls\n",
    "    dt_img_url_ls = [] # the date of the thumbnail server urls\n",
    "    if url_partition < 1:\n",
    "        url_partition = 1\n",
    "        print(\"Error! url_partition is less than 1. Set the url_partition to 1 to fix the error.\")\n",
    "    for i in range(len(start_d_utc)):\n",
    "        sdt = start_d_utc[i]\n",
    "        edt = end_d_utc[i]\n",
    "        # Add the original url\n",
    "        sdt_str = sdt.strftime(\"%Y%m%d%H%M%S\")\n",
    "        edt_str = edt.strftime(\"%Y%m%d%H%M%S\")\n",
    "        date_str = sdt_str[:8]\n",
    "        bt = \"bt=\" + sdt_str + \"&\"\n",
    "        et = \"et=\" + edt_str + \"&\"\n",
    "        l = \"l=bdrk,smell_my_city_pgh_reports,plume_\" + date_str + \"&\"\n",
    "        share_url_ls.append(et_root_url + l + bt + et + et_part)\n",
    "        dt_share_url_ls.append(date_str)\n",
    "        # Add the thumbnail server url\n",
    "        time_span = (edt - sdt) / url_partition\n",
    "        for j in range(url_partition):\n",
    "            std_j = sdt + time_span*j\n",
    "            edt_j = std_j + time_span\n",
    "            std_j_str = std_j.strftime(\"%Y%m%d%H%M%S\")\n",
    "            edt_j_str = edt_j.strftime(\"%Y%m%d%H%M%S\")\n",
    "            bt_j = \"bt=\" + std_j_str + \"&\"\n",
    "            et_j = \"et=\" + edt_j_str + \"&\"\n",
    "            rt = \"root=\" + urllib.parse.quote(et_root_url + l + bt_j + et_j + et_part, safe=\"\") + \"&\"\n",
    "            img_url_ls.append(ts_root_url + rt + ts_part)\n",
    "            dt_img_url_ls.append(date_str)\n",
    "    df_share_url = pd.DataFrame(data={\"share_url\": share_url_ls, \"date\": dt_share_url_ls})\n",
    "    df_img_url = pd.DataFrame(data={\"img_url\": img_url_ls, \"date\": dt_img_url_ls})\n",
    "\n",
    "    # return the data\n",
    "    return (df_layer, df_share_url, df_img_url, start_d, file_name)\n",
    "\n",
    "\n",
    "# Specify the starting and ending date, also the time offset\n",
    "df_layer, df_share_url, df_img_url, start_d, file_name = generate_metadata(\"2019-04-01\", \"2019-05-01\", offset_hours=3, url_partition=6)\n",
    "\n",
    "# Save rows of EarthTime CSV layers to a file\n",
    "p = \"data/earth_time.csv\"\n",
    "df_layer.to_csv(p, index=False)\n",
    "os.chmod(p, 0o777)\n",
    "\n",
    "# Save rows of share urls to a file\n",
    "p = \"data/earth_time_share_urls.csv\"\n",
    "df_share_url.to_csv(p, index=False)\n",
    "os.chmod(p, 0o777)\n",
    "\n",
    "# Save rows of thumbnail server urls to a file\n",
    "p = \"data/earth_time_thumbnail_urls.csv\"\n",
    "df_img_url.to_csv(p, index=False)\n",
    "os.chmod(p, 0o777)\n",
    "\n",
    "# Print thumbnail server urls\n",
    "for gp, df in df_img_url.groupby(\"date\"):\n",
    "    print(\"=\"*60)\n",
    "    print(gp)\n",
    "    for url in list(df[\"img_url\"]):\n",
    "        print(\"-\"*20)\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Run the simulation\n",
    "# Input:\n",
    "#   start_time_eastern: for different dates, use format \"2020-03-30 00:00\"\n",
    "#   o_file: file path to save the simulation result, e.g., \"/projects/cocalc-www.createlab.org/pardumps/test.bin\"\n",
    "#   sources: location of the sources of pollution, in an array of DispersionSource objects\n",
    "#   emit_time_hrs: affects the emission time for running each Hysplit model\n",
    "#   duration: total time (in hours) for the simulation, use 24 for a total day, use 12 for testing\n",
    "def simulate(start_time_eastern, o_file, sources, emit_time_hrs=1, duration=24):\n",
    "    print(\"=\"*100)\n",
    "    print(\"=\"*100)\n",
    "    print(\"start_time_eastern: %s\" % start_time_eastern)\n",
    "    print(\"o_file: %s\" % o_file)\n",
    "    # Run simulation and get the folder list (the generated files are cached)\n",
    "    path_list = []\n",
    "    for source in sources:\n",
    "        path_list += getMultiHourDispersionRunsParallel(\n",
    "                source,\n",
    "                parse_eastern(start_time_eastern),\n",
    "                emit_time_hrs,\n",
    "                duration,\n",
    "                HysplitModelSettings(initdModelType=InitdModelType.ParticleHV, hourlyPardump=False))\n",
    "    print(path_list)\n",
    "    # Save pdump text files (the generated files are cached)\n",
    "    pdump_txt_list = []\n",
    "    for folder in path_list:\n",
    "        if not findInFolder(folder,'*.gz') and not findInFolder(folder,'PARDUMP*.txt'):\n",
    "            pdump = findInFolder(folder,'PARDUMP.*')\n",
    "            print(pdump)\n",
    "            cmd = \"/opt/hysplit/exec/par2asc -i%s -o%s\" % (pdump, pdump+\".txt\")\n",
    "            #cmd = f\"/opt/hysplit/exec/par2asc -i{pdump} -o{pdump}.txt\"\n",
    "            if pdump.find('.txt') == -1:\n",
    "                pdump_txt_list.append(pdump+\".txt\")\n",
    "            subprocess_check(cmd)\n",
    "        if findInFolder(folder,'PARDUMP*.txt'):\n",
    "            pdump_txt = findInFolder(folder,'PARDUMP*.txt')\n",
    "            pdump_txt_list.append(pdump_txt)\n",
    "    print(pdump_txt_list)\n",
    "    # Add color\n",
    "    cmap = \"viridis\"\n",
    "    c = plt.get_cmap(cmap)\n",
    "    c.colors\n",
    "    colors = np.array(c.colors)\n",
    "    colors *= 255\n",
    "    colormap = np.uint8(colors.round())\n",
    "    colormap = colormap.reshape([1,256,3])\n",
    "    cmaps = [\n",
    "        [[250, 255, 99]],\n",
    "        [[250, 255, 99],[99, 255, 206]],\n",
    "        [[250, 255, 99],[99, 255, 206],[206, 92, 247]],\n",
    "        [[250, 255, 99],[99, 255, 206],[206, 92, 247],[255, 119, 0]]\n",
    "    ]\n",
    "    print(\"Creating %s\" % o_file)\n",
    "    points = create_multisource_bin(pdump_txt_list, o_file, len(sources), False, cmaps, duration)\n",
    "    print(\"Created %s\" % o_file)\n",
    "    os.chmod(o_file, 0o777)\n",
    "\n",
    "\n",
    "# Location of the sources of pollution\n",
    "sources = [\n",
    "    DispersionSource(name='Irvin',lat=40.328015, lon=-79.903551, minHeight=0, maxHeight=50),\n",
    "    DispersionSource(name='ET',lat=40.392967, lon=-79.855709, minHeight=0, maxHeight=50),\n",
    "    DispersionSource(name='Clairton',lat=40.305062, lon=-79.876692, minHeight=0, maxHeight=50),\n",
    "    DispersionSource(name='Cheswick',lat=40.538261, lon=-79.790391, minHeight=0, maxHeight=50)]\n",
    "\n",
    "# Prepare the list of dates for running the simulation\n",
    "start_time_eastern_all = start_d.strftime(\"%Y-%m-%d %H:%M\").values\n",
    "o_root = \"/projects/cocalc-www.createlab.org/pardumps/\"\n",
    "o_file_all = o_root + file_name.values + \".bin\"\n",
    "\n",
    "# For each date, run the simulation\n",
    "#for i in range(len(start_time_eastern_all)):\n",
    "for i in [1,22]: # for testing purposes\n",
    "    if os.path.isfile(o_file_all[i]): # skip if the file exists\n",
    "        print(\"File already exists %s\" % o_file_all[i])\n",
    "        continue\n",
    "    simulate(start_time_eastern_all[i], o_file_all[i], sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python 3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}