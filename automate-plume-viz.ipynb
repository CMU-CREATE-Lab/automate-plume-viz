{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import re, array, csv, datetime, glob, json, math, random, stat\n",
    "import pytz, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from multiprocessing.dummy import Pool\n",
    "from itertools import product\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "\n",
    "\n",
    "# This is a utility function for running other ipython notebooks\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "# Load utility functions from another ipython notebook\n",
    "root_dir = \"/projects/9ab71616-fcde-4524-bf8f-7953c669ebbb/air-src/\"\n",
    "os.chdir(root_dir + \"linRegModel/pardump_example/\")\n",
    "exec_ipynb(\"pardumpdump-randy-amy-util.ipynb\")\n",
    "os.chdir(root_dir + \"linRegModel\")\n",
    "exec_ipynb(\"./cachedHysplitRunLib.ipynb\")\n",
    "exec_ipynb(\"../../src/python-utils/utils.ipynb\")\n",
    "os.chdir(root_dir + \"automate-plume-viz/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Generate the EarthTime layers and the thumbnail server urls\n",
    "# These urls can be called later to obtain video frames\n",
    "# Input:\n",
    "#   start_date_eastern: the date to start in EST time, e.g., \"2019-01-01\"\n",
    "#   end_date_eastern: the date to start in EST time, e.g., \"2020-01-01\"\n",
    "#   offset_hour: time offset in hours, for example, if this is 3, then it starts from 12-3=9 p.m. instead of 12 a.m.\n",
    "#   url_partition: the number of partitions for the thumbnail server request for getting images of video frames\n",
    "# Output:\n",
    "#   df_layer: the pandas dataframe for the EarthTime layer document\n",
    "#   df_share_url: the pandas dataframe for the share urls\n",
    "#   df_img_url: the pandas dataframe for the thumbnail server urls to get images of video frames\n",
    "#   start_d: a pandas series of the starting datetime object in EST time\n",
    "#   file_name: a list of file names\n",
    "def generate_metadata(start_date_eastern, end_date_eastern, offset_hours=3, url_partition=4):\n",
    "    # Create rows in the EarthTime layer document\n",
    "    offset_d = pd.Timedelta(offset_hours, unit=\"h\")\n",
    "    start_d = pd.date_range(start=start_date_eastern, end=end_date_eastern, closed=\"left\", tz=\"US/Eastern\") - offset_d\n",
    "    end_d = pd.date_range(start=start_date_eastern, end=end_date_eastern, closed=\"right\", tz=\"US/Eastern\") - offset_d\n",
    "    df_template = pd.read_csv(\"data/earth_time_template.csv\")\n",
    "    df_layer = pd.concat([df_template]*len(start_d), ignore_index=True)\n",
    "    file_name = \"plume_\" + end_d.strftime(\"%Y%m%d\")\n",
    "    start_d_utc = start_d.tz_convert(\"UTC\")\n",
    "    end_d_utc = end_d.tz_convert(\"UTC\")\n",
    "    df_layer[\"Start date\"] = start_d_utc.strftime(\"%Y%m%d%H%M%S\")\n",
    "    df_layer[\"End date\"] = end_d_utc.strftime(\"%Y%m%d%H%M%S\")\n",
    "    df_layer[\"Share link identifier\"] = file_name\n",
    "    df_layer[\"Name\"] = \"PARDUMP \" + end_d.strftime(\"%Y-%m-%d\")\n",
    "    df_layer[\"URL\"] = \"https://cocalc-www.createlab.org/pardumps/\" + file_name + \".bin\"\n",
    "\n",
    "    # Create rows of share URLs\n",
    "    et_root_url = \"https://davos2019.earthtime.org/explore#\"\n",
    "    et_part = \"v=581806,708156,584252,710601,pts&ps=2400&startDwell=0&endDwell=0\"\n",
    "    ts_root_url = \"https://thumbnails-earthtime.cmucreatelab.org/thumbnail?\"\n",
    "    ts_part = \"&width=480&height=480&format=zip&fps=30&tileFormat=mp4&startDwell=0&endDwell=0&fromScreenshot&disableUI\"\n",
    "    share_url_ls = [] # EarthTime share urls\n",
    "    dt_share_url_ls = [] # the date of the share urls\n",
    "    img_url_ls = [] # thumbnail server urls\n",
    "    dt_img_url_ls = [] # the date of the thumbnail server urls\n",
    "    if url_partition < 1:\n",
    "        url_partition = 1\n",
    "        print(\"Error! url_partition is less than 1. Set the url_partition to 1 to fix the error.\")\n",
    "    for i in range(len(start_d_utc)):\n",
    "        sdt = start_d_utc[i]\n",
    "        edt = end_d_utc[i]\n",
    "        # Add the original url\n",
    "        sdt_str = sdt.strftime(\"%Y%m%d%H%M%S\")\n",
    "        edt_str = edt.strftime(\"%Y%m%d%H%M%S\")\n",
    "        date_str = sdt_str[:8]\n",
    "        bt = \"bt=\" + sdt_str + \"&\"\n",
    "        et = \"et=\" + edt_str + \"&\"\n",
    "        l = \"l=bdrk,smell_my_city_pgh_reports,plume_\" + date_str + \"&\"\n",
    "        share_url_ls.append(et_root_url + l + bt + et + et_part)\n",
    "        dt_share_url_ls.append(date_str)\n",
    "        # Add the thumbnail server url\n",
    "        time_span = (edt - sdt) / url_partition\n",
    "        for j in range(url_partition):\n",
    "            std_j = sdt + time_span*j\n",
    "            edt_j = std_j + time_span\n",
    "            std_j_str = std_j.strftime(\"%Y%m%d%H%M%S\")\n",
    "            edt_j_str = edt_j.strftime(\"%Y%m%d%H%M%S\")\n",
    "            bt_j = \"bt=\" + std_j_str + \"&\"\n",
    "            et_j = \"et=\" + edt_j_str + \"&\"\n",
    "            rt = \"root=\" + urllib.parse.quote(et_root_url + l + bt_j + et_j + et_part, safe=\"\") + \"&\"\n",
    "            img_url_ls.append(ts_root_url + rt + ts_part)\n",
    "            dt_img_url_ls.append(date_str)\n",
    "    df_share_url = pd.DataFrame(data={\"share_url\": share_url_ls, \"date\": dt_share_url_ls})\n",
    "    df_img_url = pd.DataFrame(data={\"img_url\": img_url_ls, \"date\": dt_img_url_ls})\n",
    "\n",
    "    # return the data\n",
    "    return (df_layer, df_share_url, df_img_url, start_d, file_name)\n",
    "\n",
    "\n",
    "# Specify the starting and ending date, also the time offset\n",
    "df_layer, df_share_url, df_img_url, start_d, file_name = generate_metadata(\"2019-04-01\", \"2019-05-01\", offset_hours=3, url_partition=4)\n",
    "\n",
    "# Save rows of EarthTime CSV layers to a file\n",
    "p = \"data/earth_time.csv\"\n",
    "df_layer.to_csv(p, index=False)\n",
    "os.chmod(p, 0o777)\n",
    "\n",
    "# Save rows of share urls to a file\n",
    "p = \"data/earth_time_share_urls.csv\"\n",
    "df_share_url.to_csv(p, index=False)\n",
    "os.chmod(p, 0o777)\n",
    "\n",
    "# Save rows of thumbnail server urls to a file\n",
    "p = \"data/earth_time_thumbnail_urls.csv\"\n",
    "df_img_url.to_csv(p, index=False)\n",
    "os.chmod(p, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Run the simulation\n",
    "# Input:\n",
    "#   start_time_eastern: for different dates, use format \"2020-03-30 00:00\"\n",
    "#   o_file: file path to save the simulation result, e.g., \"/projects/cocalc-www.createlab.org/pardumps/test.bin\"\n",
    "#   sources: location of the sources of pollution, in an array of DispersionSource objects\n",
    "#   emit_time_hrs: affects the emission time for running each Hysplit model\n",
    "#   duration: total time (in hours) for the simulation, use 24 for a total day, use 12 for testing\n",
    "def simulate(start_time_eastern, o_file, sources, emit_time_hrs=1, duration=24):\n",
    "    print(\"=\"*100)\n",
    "    print(\"=\"*100)\n",
    "    print(\"start_time_eastern: %s\" % start_time_eastern)\n",
    "    print(\"o_file: %s\" % o_file)\n",
    "\n",
    "    # Run simulation and get the folder list (the generated files are cached)\n",
    "    path_list = []\n",
    "    for source in sources:\n",
    "        path_list += getMultiHourDispersionRunsParallel(\n",
    "                source,\n",
    "                parse_eastern(start_time_eastern),\n",
    "                emit_time_hrs,\n",
    "                duration,\n",
    "                HysplitModelSettings(initdModelType=InitdModelType.ParticleHV, hourlyPardump=False))\n",
    "    print(path_list)\n",
    "\n",
    "    # Save pdump text files (the generated files are cached)\n",
    "    pdump_txt_list = []\n",
    "    for folder in path_list:\n",
    "        if not findInFolder(folder,'*.gz') and not findInFolder(folder,'PARDUMP*.txt'):\n",
    "            pdump = findInFolder(folder,'PARDUMP.*')\n",
    "            print(pdump)\n",
    "            cmd = \"/opt/hysplit/exec/par2asc -i%s -o%s\" % (pdump, pdump+\".txt\")\n",
    "            #cmd = f\"/opt/hysplit/exec/par2asc -i{pdump} -o{pdump}.txt\"\n",
    "            if pdump.find('.txt') == -1:\n",
    "                pdump_txt_list.append(pdump+\".txt\")\n",
    "            subprocess_check(cmd)\n",
    "        if findInFolder(folder,'PARDUMP*.txt'):\n",
    "            pdump_txt = findInFolder(folder,'PARDUMP*.txt')\n",
    "            pdump_txt_list.append(pdump_txt)\n",
    "    print(pdump_txt_list)\n",
    "\n",
    "    # Add color\n",
    "    cmap = \"viridis\"\n",
    "    c = plt.get_cmap(cmap)\n",
    "    c.colors\n",
    "    colors = np.array(c.colors)\n",
    "    colors *= 255\n",
    "    colormap = np.uint8(colors.round())\n",
    "    colormap = colormap.reshape([1,256,3])\n",
    "    cmaps = [\n",
    "        [[250, 255, 99]],\n",
    "        [[250, 255, 99],[99, 255, 206]],\n",
    "        [[250, 255, 99],[99, 255, 206],[206, 92, 247]],\n",
    "        [[250, 255, 99],[99, 255, 206],[206, 92, 247],[255, 119, 0]]\n",
    "    ]\n",
    "    print(\"Creating %s\" % o_file)\n",
    "    points = create_multisource_bin(pdump_txt_list, o_file, len(sources), False, cmaps, duration)\n",
    "    print(\"Created %s\" % o_file)\n",
    "    os.chmod(o_file, 0o777)\n",
    "\n",
    "\n",
    "# Location of the sources of pollution\n",
    "sources = [\n",
    "    DispersionSource(name='Irvin',lat=40.328015, lon=-79.903551, minHeight=0, maxHeight=50),\n",
    "    DispersionSource(name='ET',lat=40.392967, lon=-79.855709, minHeight=0, maxHeight=50),\n",
    "    DispersionSource(name='Clairton',lat=40.305062, lon=-79.876692, minHeight=0, maxHeight=50),\n",
    "    DispersionSource(name='Cheswick',lat=40.538261, lon=-79.790391, minHeight=0, maxHeight=50)]\n",
    "\n",
    "# Prepare the list of dates for running the simulation\n",
    "start_time_eastern_all = start_d.strftime(\"%Y-%m-%d %H:%M\").values\n",
    "o_root = \"/projects/cocalc-www.createlab.org/pardumps/\"\n",
    "o_file_all = o_root + file_name.values + \".bin\"\n",
    "\n",
    "# For each date, run the simulation\n",
    "#for i in range(len(start_time_eastern_all)):\n",
    "for i in [0,1,2,3,4,5,6,22]: # for testing purposes\n",
    "    if os.path.isfile(o_file_all[i]): # skip if the file exists\n",
    "        print(\"File already exists %s\" % o_file_all[i])\n",
    "        continue\n",
    "    try:\n",
    "        simulate(start_time_eastern_all[i], o_file_all[i], sources)\n",
    "    except Exception as ex:\n",
    "        print(\"\\t{%s} %s\\n\" % (ex, o_file_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Call the thumbnail server to generate and get video frames\n",
    "# Then save the video frames\n",
    "# Input:\n",
    "#   df_img_url: the pandas dataframe generated by using the generate_metadata function\n",
    "#   dir_p: the folder path for saving the files\n",
    "#   num_try: the number of times that the function has been called\n",
    "def get_frames(df_img_url, dir_p=\"data/rgb/\", num_try=0):\n",
    "    print(\"=\"*100)\n",
    "    print(\"=\"*100)\n",
    "    print(\"This function has been called for %d times.\" % num_try)\n",
    "    if num_try > 30:\n",
    "        print(\"Terminate the recursive call due to many errors. Please check manually.\")\n",
    "        return\n",
    "    num_errors = 0\n",
    "    for dt, df in df_img_url.groupby(\"date\"):\n",
    "        print(\"=\"*60)\n",
    "        print(dt + \"\\n\")\n",
    "        # Construct the lists of urls and file paths\n",
    "        img_url_list = list(df[\"img_url\"])\n",
    "        arg_list = []\n",
    "        n = len(img_url_list)\n",
    "        dir_p_dt = dir_p + dt + \"/\"\n",
    "        check_and_create_dir(dir_p) # need this line to set the permission\n",
    "        check_and_create_dir(dir_p_dt)\n",
    "        for i in range(n):\n",
    "            arg_list.append((img_url_list[i], dir_p_dt + str(i) + \".zip\", i))\n",
    "        # Download the files in parallel\n",
    "        result = Pool(n).starmap(urlretrieve_worker, arg_list)\n",
    "        for r in result:\n",
    "            if r: num_errors += 1\n",
    "    if num_errors > 0:\n",
    "        print(\"=\"*60)\n",
    "        print(\"Has %d errors. Need to do again.\" % num_errors)\n",
    "        num_try += 1\n",
    "        get_frames(df_img_url, num_try=num_try)\n",
    "    else:\n",
    "        print(\"DONE\")\n",
    "\n",
    "\n",
    "# The worker for getting the video frames\n",
    "# Input:\n",
    "#   url: the url for getting the frames\n",
    "#   file_p: the path for saving the file\n",
    "#   idx: the index of the worker\n",
    "def urlretrieve_worker(url, file_p, idx):\n",
    "    time.sleep(idx) # sleep to prevent calling the server too fast\n",
    "    error = False\n",
    "    if os.path.isfile(file_p): # skip if the file exists\n",
    "        print(\"\\t{File exists} %s\\n\" % file_p)\n",
    "        return error\n",
    "    try:\n",
    "        print(\"\\t{Request} %s\\n\" % url)\n",
    "        urllib.request.urlretrieve(url, file_p)\n",
    "        os.chmod(file_p, 0o777)\n",
    "        print(\"\\t{Done} %s\\n\" % url)\n",
    "    except Exception as ex:\n",
    "        print(\"\\t{%s} %s\\n\" % (ex, url))\n",
    "        error = True\n",
    "    return error\n",
    "\n",
    "\n",
    "# Check if a directory exists, if not, create it\n",
    "def check_and_create_dir(path):\n",
    "    if path is None: return\n",
    "    dir_name = os.path.dirname(path)\n",
    "    if dir_name != \"\" and not os.path.exists(dir_name):\n",
    "        try: # this is used to prevent race conditions during parallel computing\n",
    "            os.makedirs(dir_name)\n",
    "            os.chmod(dir_name, 0o777)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "\n",
    "# Make sure that the dates have the hysplit simulation results\n",
    "date_has_hysplit = []\n",
    "for idx, row in df_share_url.iterrows():\n",
    "    if os.path.isfile(o_root + \"plume_\" + row[\"date\"] + \".bin\"):\n",
    "        date_has_hysplit.append(row[\"date\"])\n",
    "get_frames(df_img_url[df_img_url[\"date\"].isin(date_has_hysplit)], dir_p=\"data/rgb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Unzip the video frames and rename them to the correct datetime\n",
    "# Input:\n",
    "#   dir_p: path to the folder that has the zip file for one day's data\n",
    "#   offset_hour: time offset in hours, for example, if this is 3, then it starts from 12-3=9 p.m. instead of 12 a.m.\n",
    "def unzip_and_rename(in_dir_p, out_dir_p, offset_hours=3):\n",
    "    # Compute the number of partitions\n",
    "    num_partitions = 0\n",
    "    for fn in get_all_file_names_in_folder(in_dir_p):\n",
    "        if \".zip\" not in fn: continue\n",
    "        num_partitions += 1\n",
    "\n",
    "    # Unzip each partition\n",
    "    start_dt_str = re.findall(r\"\\d{8}\", in_dir_p)[0]\n",
    "    start_dt = datetime.datetime.strptime(start_dt_str, \"%Y%m%d\")\n",
    "    start_dt = pytz.timezone(\"US/Eastern\").localize(start_dt)\n",
    "    start_dt = start_dt - pd.Timedelta(offset_hours, unit=\"h\")\n",
    "    time_span = pd.Timedelta(24 / num_partitions, unit=\"h\")\n",
    "    num_files_per_partition = 0\n",
    "    for i in range(num_partitions):\n",
    "        start_dt_partition = start_dt + time_span * i\n",
    "        p_zip = in_dir_p + \"%d.zip\" % i\n",
    "        p_unzip = in_dir_p + str(i) + \"/\"\n",
    "        del_dir(p_unzip)\n",
    "        print(\"Extract \" + p_zip + \" to \" + p_unzip)\n",
    "        with ZipFile(p_zip, \"r\") as zip_obj:\n",
    "            zip_obj.extractall(p_unzip)\n",
    "            os.chmod(p_unzip, 0o777)\n",
    "            for dn in get_all_dir_names_in_folder(p_unzip):\n",
    "                os.chmod(p_unzip + dn, 0o777)\n",
    "            # Count the number of png files\n",
    "            fn_list = get_all_file_names_in_folder(p_unzip + \"frames/\")\n",
    "            if num_files_per_partition == 0:\n",
    "                for fn in fn_list:\n",
    "                    if \"frame\" in fn and \".png\" in fn:\n",
    "                        num_files_per_partition += 1\n",
    "            # Loop and rename the files\n",
    "            time_span_frame = pd.Timedelta(time_span/(num_files_per_partition - 1), unit=\"h\")\n",
    "            for fn in fn_list:\n",
    "                frame_number = int(re.findall(r\"\\d{6}\", fn)[0]) - 1\n",
    "                frame_epochtime = start_dt_partition + time_span_frame * frame_number\n",
    "                frame_epochtime = round(frame_epochtime.timestamp())\n",
    "                new_fn = str(frame_epochtime) + \".png\"\n",
    "                os.rename(p_unzip + \"frames/\" + fn, p_unzip + \"frames/\" + new_fn)\n",
    "\n",
    "    # Put files in one folder\n",
    "    del_dir(out_dir_p)\n",
    "    check_and_create_dir(out_dir_p)\n",
    "    for i in range(num_partitions):\n",
    "        p = in_dir_p + str(i) + \"/frames/\"\n",
    "        for fn in get_all_file_names_in_folder(p):\n",
    "            os.rename(p + fn, out_dir_p + fn)\n",
    "        del_dir(in_dir_p + str(i))\n",
    "    print(\"DONE\")\n",
    "\n",
    "\n",
    "# Convert the UTC timezone to another timezone\n",
    "def utc_to_other(utc_dt, tz_str):\n",
    "    return utc_dt.replace(tzinfo=pytz.utc).astimezone(tz=pytz.timezone(tz_str))\n",
    "\n",
    "\n",
    "# Delete a directory and all its contents\n",
    "def del_dir(dir_p):\n",
    "    if not os.path.isdir(dir_p): return\n",
    "    try:\n",
    "        shutil.rmtree(dir_p)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "# Return a list of all files in a folder\n",
    "def get_all_file_names_in_folder(path):\n",
    "    return [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "\n",
    "# Return a list of all directories in a folder\n",
    "def get_all_dir_names_in_folder(path):\n",
    "    return [f for f in listdir(path) if isdir(join(path, f))]\n",
    "\n",
    "\n",
    "for dn in get_all_dir_names_in_folder(\"data/rgb/\"):\n",
    "    in_dir_p = \"data/rgb/\" + dn + \"/\"\n",
    "    unzip_and_rename(in_dir_p, in_dir_p+\"frames/\", offset_hours=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python 3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}